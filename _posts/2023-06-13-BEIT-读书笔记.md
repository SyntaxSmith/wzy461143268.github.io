---
layout: mypost
title: BEIT-读书笔记
categories: [论文阅读记录]
---

# BEiT

![BEiT结构图](BEiT.png)

任务目标，使用掩盖过的encoding vector 预测visual tokens（未遮盖）

- 将224x224的图片划分成16x16个patches，flatten输入BERT
- visual tokens使用dVAE生成

将flatten向量喂给L层Transformer网络，针对最终的隐向量${h_{i}^L}$ 看作输入向量的编码表示，使用softmax分类器针对被遮盖位置的隐向量做预测，产生对应位置token的prediction

# CycleMLP

​	CycleMLP 相较于现代方法有两个优势。

- 它可以处理各种图像尺寸；
- 通过使用本地窗口，它可以实现与图像尺寸的线性计算复杂度。

CycleMLP 在目标检测、实例分割和语义分割领域取得了竞争性的结果。特别是，CycleMLP-Tiny 在 ADE20K 数据集上以更少的 FLOPs 超越了 Swin-Tiny，其 mIoU 比 Swin-Tiny 高出 1.3%。此外，CycleMLP 还展现了在 ImageNet-C 数据集上的出色零样本鲁棒性。

![image-20230614133725225](Cycle对比表.png)

![image-20230614134349666](Cycle示意图.png)

Cycle FC享有channel FC的优势，即以任意分辨率和线性计算复杂性接收输入，同时扩大其上下文聚合的感受野，具体来说，**Cycle FC采样点沿通道维度呈循环样式**，这样Cycle FC和channel FC具有相同的复杂性（参数数量和FLOP）同时增加了感受野，为此，采用Cycle FC代替Spatial FC进行空间上下文聚合（即token mixing），并为识别和密集预测任务构建一系列类似的MLP模型。

## 结构

- patch embedding，windows=7，stride=4，产出特征为$$ \frac{H}{4} \times \frac{W}{4} \times C$$
- CycleMLP Block 

![image-20230614161903428](MLP结构图.png)
